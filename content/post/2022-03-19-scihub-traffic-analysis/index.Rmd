---
title: "探索定西市的 Sci-Hub 流量之谜"
author: "袁凡"
date: "2022-03-19"
categories:
  - 统计应用
tags:
  - R 语言
  - echarts4r
  - 流量分析
meta_extra: "审稿：于淼、黄湘云；编辑：黄湘云"
output:
  blogdown::html_page:
    toc: false
link-citations: true
bibliography:
  - packages.bib
# thumbnail: xxx.png
description: ""
forum_id: 422978
slug: scihub-traffic-analysis
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
```

某年某月某日，地球上的一个人类忽然恰好想打开朋友圈看看大家都在做什么，于是恰好翻到一条视频，配文"我们的工作，将会写在人类的历史上"，该名人类恰好在这瞬间产生了一分好奇心，接着点开视频看到一只叼着钥匙的黑乌鸦，于是就恰好发现了[Sci-Hub](https://sci-hub.se/)这个神奇的网站。其实这位人类平时只是一条在死水里躺平的咸鱼，但是又恰好被一位统计之都的编辑[黄湘云](https://xiangyun.rbind.io/)捞起来做点事，于是顺理成章地诞生了[这篇讨论帖](https://d.cosx.org/d/422978-sci-hubdoi)和这篇文章。 

Sci-Hub 这个网站的神奇之处在于，只要是网站数据库中收录了的文献，那么只需要输入文献相关的 DOI、PMID 或者直接链接就可以看到文献内容。Sci-Hub 网站的文献是完全免费的，它建立的目标是**to remove all barriers in the way of science（消除科学前进之路上的所有障碍）**。Sci-Hub 网站上目前公开了三个时间段的下载日志数据。第一个是 2011-09-22 至 2013-10-14 的 Sci-Hub 日志[^1]，一共有4728537条记录，当时下载量第一的国家是俄罗斯，下载量有1491679，而在中国 Sci-Hub 还鲜为人知，所以中国的总下载量仅82634，当年为了保护 Sci-Hub，所有来自美国的访问都被阻止。第二个是2015-09-01 至 2016-02-29的 Sci-Hub 日志[^2]，一共有27819965条记录（约2.7千万条）。第三个是2017年的 Sci-Hub 日志[^3]，一共有150875862条记录（约1.5亿条）。

```{r}
library(data.table)
library(echarts4r)

data0 <- fread("data/scihub_country_freq.csv")

cns <- countrycode::codelist$country.name.en
cns <- data.frame(
  country = cns,
  value = runif(length(cns), 1, 100)
)

cns <- as.data.table(cns)

data0 <- cns[data0, on = "country"]

data0 <- data0[is.na(value) == FALSE, -c("value")]

data0.add <- data.table(
  country = c("Hong Kong SAR China", "Macao SAR China"),
  freq = c(1617000, 52953)
)

data0 <- rbind(data0, data0.add)

data0 |>
  e_charts(country) |> # 国家
  e_map(freq) |>
  e_visual_map(freq, show = FALSE) |>
  e_tooltip(trigger = "item") |>
  e_title("Sci-Hub 2017年下载量世界分布图", left = "center")
```


下载下来2017年的数据包，解压以后的大小约14G，共包含8个字段。

-   time：时间戳，格式是"yyyy-MM-dd HH:mm:ss"。
-   doi：一篇论文的唯一标识编码，可据此用 **rcrossref** 包 [@rcrossref]获取文章的元数据。
-   ip：用户设备 IP 地址的唯一标识，脱敏后用数字表示。
-   user：用户的唯一标识，脱敏后用数字表示。
-   country/city：根据 GeoIP 数据库解析 IP 地址得到的用户所在国家和城市。
-   latitude/longitude：经度和纬度。

这份数据的庐山真面目大致如下。

|time|doi|ip|user|country|city|latitude|longitude|
|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|
|2017-01-01 00:09:30|10.1007/s00134-016-4523-0|583|671|China|Beijing|39.904211|116.407395|
|2017-01-01 00:03:27|10.1080/15567036.2015.1057657|271|298|China|Shanghai|31.230416|121.473701|
|2017-01-01 00:09:52|10.1002/ejoc.201601322|604|692|China|Dingxi Shi|35.580663|104.626282|
|2017-01-01 00:02:17|10.1159/000107370|193|210|Taiwan|N/A|25.0378259|121.5212991|
|2017-01-01 00:21:47|10.1108/13555850510672386|1046|1243| Hong Kong|N/A|22.2500|114.1667|
|2017-01-01 00:05:45|10.1007/BF02724185|402|453|India|Gugal Pimpari|19.9978027|77.0057059|
|2017-01-01 00:23:02|10.1002/smll.201000908|1088|1296|United States|Los Angeles|34.0522342|-118.2436849|
|2017-01-01 00:40:38|10.1021/nl104555t|1088|1296|United States|Los Angeles|34.0522342|-118.2436849|
|2017-01-01 00:02:05|10.1109/tcsii.2016.2608866|175|188|Brazil|Rio de Janeiro|-22.9068467|-43.1728965|
|2017-01-01 00:00:01|10.1049/iet-gtd.2014.1045|6|6|Iran|N/A |N/A |N/A|

一般情况下一份数据拿到手，可以先从三个基本面入手分析：总量概览、区域对比、趋势分析。因为对中国的地理和国情更熟悉一些，于是先取出中国的数据来看一看。按城市或地区汇总下载量，再按总量从高到低排序，意外发现总下载量第三名是一个名不见经传的小城市：定西市。

| 排名 | 城市或地区 | 2017年 Sci-Hub 总下载量 |
|:----:|:----------:|:-----------------------:|
|  1   |    北京    |         3807399         |
|  2   |    上海    |         2915865         |
|  3   |    定西    |         2147668         |
|  4   |    台湾    |         1999026         |
|  5   |    香港    |         1617000         |
|  6   |    广州    |         1365568         |
|  7   |    南京    |         1305058         |
|  8   |    武汉    |         1050703         |
|  9   |    杭州    |         962155          |
|  10  |    成都    |         904620          |

坛友[于淼](https://yufree.cn/)认为定西市没有知名研究机构，不太可能是自然流量，他猜测定西市的流量可能来源于其他地方，并且给出了验证思路。做数据分析应秉承"大胆假设、小心验证"的信念，一边开脑洞，一边探索数据。于是乎，于淼脑洞成为了随后探索定西市 Sci-Hub 流量之谜的起点。

# 1. 是否自然流量

本（咸）文（鱼）作（人）者（类）认为由正常人类产生的网站流量属于自然流量，由除正常人类以外的非正常手段产生的网站流量属于非自然流量。倘若定西市全部流量都不是由正常人类产生的，那么时间上的趋势应当是相对平缓的。因为正常人类不会一年365天每天都下载文献来读，总要放假休息，也不会一天24小时都读文献，总要吃饭睡觉，所以在正常人类休息的时间就会形成流量的波谷。

取出全中国总下载量排名前5的城市或地区的数据，先按天汇总下载量，观察其随时间变化的趋势。从下图中可以看到，这5个城市或地区的数据表现出了一些完全一致的现象，这些共同的表象至少说明定西市的流量不会全是非自然的。

-   都在1月底2月初出现波谷，正好对应中国传统春节假期的时间。
-   都缺失 2017-04-21 至 2017-04-29 和 2017-10-07 至 2017-10-29 的数据，都在 2017-12-16 出现最小值。
-   都有明显的 7 日周期性波动。

```{r}
data1 <- fread("data/cn_by_day.csv")
data1$date <- as.Date(data1$date)

data1 |>
  e_charts(date) |>
  e_line(bj, name = "北京") |>
  e_line(sh, name = "上海") |>
  e_line(dx, name = "定西") |>
  e_line(tw, name = "台湾") |>
  e_line(hk, name = "香港") |>
  # e_line(gz, name = "广州") |>
  # e_line(nj, name = "南京") |>
  # e_line(wh, name = "武汉") |>
  # e_line(hz, name = "杭州") |>
  # e_line(cd, name = "成都") |>
  # e_line(scihub, name = "Sci-Hub") |>
  e_tooltip(trigger = "axis") |>
  e_y_axis(
    # name = "下载量",
    axisLine = list(
      show = TRUE,
      symbol = c("none", "arrow")
    )
  ) |>
  e_legend(
    type = "scroll",
    selector = c("all", "inverse"),
    selectorPosition = "end",
    top = 25
  ) |>
  e_title("Sci-Hub 2017年每日下载量", left = "center") |>
  e_datazoom()
```

接着按小时汇总下载量，再观察其随时间变化的趋势。从下图中可以看到，这5个城市或地区的数据仍然表现出了一些完全一致的现象：都出现了三个峰值。其他4个城市或地区的流量均是出现两个较高的波峰和一个较矮的波峰，而**定西的流量出现了三个差不多高的波峰**。

-   北京：在5点、11点出现较高波峰，16点出现较矮波峰。
-   上海：在5点、10点出现较高波峰，16点出现较矮波峰。
-   台湾：在6点、10-11点出现较高波峰，17点出现较矮波峰。
-   香港：在5-6点、10-11点出现较高波峰，16-17点出现较矮波峰。
-   定西：在5点、11点、16点出现三个波峰，且16点的波峰最高。

```{r}
data2 <- fread("data/cn_by_hour.csv")

data2$hour <- as.character(data2$hour)

data2 |>
  e_charts(hour) |>
  e_line(bj, name = "北京", smooth = TRUE) |>
  e_line(sh, name = "上海", smooth = TRUE) |>
  e_line(dx, name = "定西", smooth = TRUE) |>
  e_line(tw, name = "台湾", smooth = TRUE) |>
  e_line(hk, name = "香港", smooth = TRUE) |>
  # e_line(gz, name = "广州",smooth = TRUE) |>
  # e_line(nj, name = "南京",smooth = TRUE) |>
  # e_line(wh, name = "武汉",smooth = TRUE) |>
  # e_line(hz, name = "杭州",smooth = TRUE) |>
  # e_line(cd, name = "成都",smooth = TRUE) |>
  # e_line(scihub, name = "Sci-Hub") |>
  e_tooltip(trigger = "axis") |>
  e_x_axis(axisLine = list(interval = 0)) |>
  e_y_axis(
    # name = "下载量",
    axisLine = list(
      show = TRUE,
      symbol = c("none", "arrow")
    )
  ) |>
  e_legend(
    type = "scroll",
    selector = c("all", "inverse"),
    selectorPosition = "end",
    top = 25
  ) |>
  e_title("Sci-Hub 2017年24小时下载量", left = "center") |>
  e_mark_point(c("北京", "上海", "定西", "台湾", "香港"), data = list(type = "max")) # 批量标注最大值
```

话说回来，凌晨5点就爬起来读论文是正常人能干出来的事嘛？

倒也不是完全不可能......一定有人听说过篮球明星科比的名言："你见过凌晨四点的洛杉矶嘛！"也一定有人读过 C.R RAO 的《统计与真理：怎样运用偶然性》，此书扉页写着："本书谨献给引导我探求知识的母亲 A. Laxmikanthamma。在我年少时，母亲每天早上四点起床，为我点上油灯，使我能在安静的早晨精力充沛地用功读书。"由此可见，极个别的人5点读论文是正常的。只不过，大多数人都在5点读论文就不正常了。

也许是打开的方式不对，根据日期拆分出工作日和周末，再按小时汇总下载量，然后观察其随时间变化的趋势。如果只看“工作日”的数据，那么整体趋势和之前基本一致，还是只有定西第三个波峰最高，其余4个城市或地区的第三个波峰最低。但如果只看“周末”的数据，一切又似乎有些不同，台湾和定西一样是第三个波峰最高，其余3个城市或地区则变成三个差不多高的波峰。

```{r}
data3 <- fread("data/cn_by_wday2_hour.csv", encoding = "UTF-8")

data3 <- data3[order(hour), ]
data3$hour <- as.character(data3$hour)

data3[type %in% c("北京", "上海", "定西", "台湾", "香港"), ] |>
  group_by(type) |>
  e_charts(hour, timeline = TRUE) |>
  e_line(freq_1, name = "周末", smooth = TRUE) |>
  e_line(freq_2, name = "工作日", smooth = TRUE) |>
  e_tooltip(trigger = "axis") |>
  e_x_axis(axisLine = list(interval = 0)) |>
  e_y_axis(name = "下载量", axisLine = list(
    show = TRUE,
    symbol = c("none", "arrow")
  )) |>
  e_legend(
    right = 30,
    orient = "vertical",
    itemGap = 15
  ) |>
  e_title(left = "center", top = 5) |>
  e_timeline_serie(title = list(
    list(
      text = "Sci-Hub 2017年24小时下载量",
      textStyle = list(
        fontWeight = "normal",
        fontSize = 20
      ),
      subtext = "北京",
      subtextStyle = list(
        fontWeight = "bold",
        fontSize = 30
      )
    ),
    list(text = "Sci-Hub 2017年24小时下载量", subtext = "定西"),
    list(text = "Sci-Hub 2017年24小时下载量", subtext = "上海"),
    list(text = "Sci-Hub 2017年24小时下载量", subtext = "台湾"),
    list(text = "Sci-Hub 2017年24小时下载量", subtext = "香港")
  ))
```

到这里，可以得到一个初步的结论：定西市的流量和其他几个城市或地区一样，多数是由正常人类产生的自然流量，但是不能排除混入非自然流量的可能性。可是，为什么大家都要在5点读论文呢？

# 2. 一天三个波峰是否正常

既然出现了"5点是论文下载流量高峰"的表象，却又和常识不相符，不如放大数据范围，我们看看其他国家、其他城市或地区是否也存在类似情况。选出 Sci-Hub 2017年论文下载量最高的前五个国家，每个国家选出下载量最高的前五个城市或地区，按小时汇总下载量，观察流量随着时间变化的趋势。

```{r}
data4 <- fread("data/country_5_city_15_24hour.csv", encoding = "UTF-8")

chart4 <- dcast(data4, hour ~ city, value.var = "freq")

chart4$hour <- as.character(chart4$hour)

# 中国 北京
e1 <- chart4 |>
  e_charts(hour, height = 60) |>
  e_line(Beijing, color = "#5460c6", smooth = TRUE) |>
  e_y_axis(show = FALSE) |>
  e_legend(show = FALSE) |>
  e_title(
    text = "北京",
    textStyle = list(
      fontWeight = "bold",
      fontSize = 20
    ),
    subtext = "中国",
    subtextStyle = list(
      fontWeight = "normal",
      fontSize = 20
    ),
    left = "left",
    top = 5,
    itemGap = 5
  ) |>
  e_tooltip(
    trigger = "axis"
  ) |>
  e_grid(bottom = 20, top = 0)

# 中国 上海
e2 <- chart4 |>
  e_charts(hour, height = 60) |>
  e_line(Shanghai, color = "#5460c6", smooth = TRUE) |>
  e_y_axis(show = FALSE) |>
  e_legend(show = FALSE) |>
  e_title(
    text = "上海",
    textStyle = list(
      fontWeight = "bold",
      fontSize = 20
    ),
    subtext = "中国",
    subtextStyle = list(
      fontWeight = "normal",
      fontSize = 20
    ),
    left = "left",
    top = 5,
    itemGap = 5
  ) |>
  e_tooltip(
    trigger = "axis"
  ) |>
  e_grid(bottom = 20, top = 0)

# 中国 定西
e3 <- chart4 |>
  e_charts(hour, height = 60) |>
  e_line(`Dingxi Shi`, color = "#5460c6", smooth = TRUE) |>
  e_y_axis(show = FALSE) |>
  e_legend(show = FALSE) |>
  e_title(
    text = "定西",
    textStyle = list(
      fontWeight = "bold",
      fontSize = 20
    ),
    subtext = "中国",
    subtextStyle = list(
      fontWeight = "normal",
      fontSize = 20
    ),
    left = "left",
    top = 5,
    itemGap = 5
  ) |>
  e_tooltip(
    trigger = "axis"
  ) |>
  e_grid(bottom = 20, top = 0)

# 中国 台湾
e4 <- chart4 |>
  e_charts(hour, height = 60) |>
  e_line(Taiwan, color = "#5460c6", smooth = TRUE) |>
  e_y_axis(show = FALSE) |>
  e_legend(show = FALSE) |>
  e_title(
    text = "台湾",
    textStyle = list(
      fontWeight = "bold",
      fontSize = 20
    ),
    subtext = "中国",
    subtextStyle = list(
      fontWeight = "normal",
      fontSize = 20
    ),
    left = "left",
    top = 5,
    itemGap = 5
  ) |>
  e_tooltip(
    trigger = "axis"
  ) |>
  e_grid(bottom = 20, top = 0)

# 中国 香港
e5 <- chart4 |>
  e_charts(hour, height = 60) |>
  e_line(`Hong Kong`, color = "#5460c6", smooth = TRUE) |>
  e_y_axis(show = FALSE) |>
  e_legend(show = FALSE) |>
  e_title(
    text = "香港",
    textStyle = list(
      fontWeight = "bold",
      fontSize = 20
    ),
    subtext = "中国",
    subtextStyle = list(
      fontWeight = "normal",
      fontSize = 20
    ),
    left = "left",
    top = 5,
    itemGap = 5
  ) |>
  e_tooltip(
    trigger = "axis"
  ) |>
  e_grid(bottom = 20, top = 0)

# 印度 Gugal Pimpari
e6 <- chart4 |>
  e_charts(hour, height = 60) |>
  e_line(`Gugal Pimpari`, color = "#91cc75", smooth = TRUE) |>
  e_y_axis(show = FALSE) |>
  e_legend(show = FALSE) |>
  e_title(
    text = "Gugal Pimpari",
    textStyle = list(
      fontWeight = "bold",
      fontSize = 20
    ),
    subtext = "印度",
    subtextStyle = list(
      fontWeight = "normal",
      fontSize = 20
    ),
    left = "left",
    top = 5,
    itemGap = 5
  ) |>
  e_tooltip(
    trigger = "axis"
  ) |>
  e_grid(bottom = 20, top = 0)

# 印度 New Delhi
e7 <- chart4 |>
  e_charts(hour, height = 60) |>
  e_line(`New Delhi`, color = "#91cc75", smooth = TRUE) |>
  e_y_axis(show = FALSE) |>
  e_legend(show = FALSE) |>
  e_title(
    text = "New Delhi",
    textStyle = list(
      fontWeight = "bold",
      fontSize = 20
    ),
    subtext = "印度",
    subtextStyle = list(
      fontWeight = "normal",
      fontSize = 20
    ),
    left = "left",
    top = 5,
    itemGap = 5
  ) |>
  e_tooltip(
    trigger = "axis"
  ) |>
  e_grid(bottom = 20, top = 0)


# 印度 Chennai
e8 <- chart4 |>
  e_charts(hour, height = 60) |>
  e_line(Chennai, color = "#91cc75", smooth = TRUE) |>
  e_y_axis(show = FALSE) |>
  e_legend(show = FALSE) |>
  e_title(
    text = "Chennai",
    textStyle = list(
      fontWeight = "bold",
      fontSize = 20
    ),
    subtext = "印度",
    subtextStyle = list(
      fontWeight = "normal",
      fontSize = 20
    ),
    left = "left",
    top = 5,
    itemGap = 5
  ) |>
  e_tooltip(
    trigger = "axis"
  ) |>
  e_grid(bottom = 20, top = 0)

# 印度 Bengaluru
e9 <- chart4 |>
  e_charts(hour, height = 60) |>
  e_line(Bengaluru, color = "#91cc75", smooth = TRUE) |>
  e_y_axis(show = FALSE) |>
  e_legend(show = FALSE) |>
  e_title(
    text = "Bengaluru",
    textStyle = list(
      fontWeight = "bold",
      fontSize = 20
    ),
    subtext = "印度",
    subtextStyle = list(
      fontWeight = "normal",
      fontSize = 20
    ),
    left = "left",
    top = 5,
    itemGap = 5
  ) |>
  e_tooltip(
    trigger = "axis"
  ) |>
  e_grid(bottom = 20, top = 0)

# 印度 Hyderabad
e10 <- chart4 |>
  e_charts(hour, height = 60) |>
  e_line(Hyderabad, color = "#91cc75", smooth = TRUE) |>
  e_y_axis(show = FALSE) |>
  e_legend(show = FALSE) |>
  e_title(
    text = "Hyderabad",
    textStyle = list(
      fontWeight = "bold",
      fontSize = 20
    ),
    subtext = "印度",
    subtextStyle = list(
      fontWeight = "normal",
      fontSize = 20
    ),
    left = "left",
    top = 5,
    itemGap = 5
  ) |>
  e_tooltip(
    trigger = "axis"
  ) |>
  e_grid(bottom = 20, top = 0)


# 美国 Los Angeles
e11 <- chart4 |>
  e_charts(hour, height = 60) |>
  e_line(`Los Angeles`, color = "#fac858", smooth = TRUE) |>
  e_y_axis(show = FALSE) |>
  e_legend(show = FALSE) |>
  e_title(
    text = "Los Angeles",
    textStyle = list(
      fontWeight = "bold",
      fontSize = 20
    ),
    subtext = "美国",
    subtextStyle = list(
      fontWeight = "normal",
      fontSize = 20
    ),
    left = "left",
    top = 5,
    itemGap = 5
  ) |>
  e_tooltip(
    trigger = "axis"
  ) |>
  e_grid(bottom = 20, top = 0)

# 美国 Chicago
e12 <- chart4 |>
  e_charts(hour, height = 60) |>
  e_line(Chicago, color = "#fac858", smooth = TRUE) |>
  e_y_axis(show = FALSE) |>
  e_legend(show = FALSE) |>
  e_title(
    text = "Chicago",
    textStyle = list(
      fontWeight = "bold",
      fontSize = 20
    ),
    subtext = "美国",
    subtextStyle = list(
      fontWeight = "normal",
      fontSize = 20
    ),
    left = "left",
    top = 5,
    itemGap = 5
  ) |>
  e_tooltip(
    trigger = "axis"
  ) |>
  e_grid(bottom = 20, top = 0)

# 美国 Wilmington
e13 <- chart4 |>
  e_charts(hour, height = 60) |>
  e_line(Wilmington, color = "#fac858", smooth = TRUE) |>
  e_y_axis(show = FALSE) |>
  e_legend(show = FALSE) |>
  e_title(
    text = "Wilmington",
    textStyle = list(
      fontWeight = "bold",
      fontSize = 20
    ),
    subtext = "美国",
    subtextStyle = list(
      fontWeight = "normal",
      fontSize = 20
    ),
    left = "left",
    top = 5,
    itemGap = 5
  ) |>
  e_tooltip(
    trigger = "axis"
  ) |>
  e_grid(bottom = 20, top = 0)

# 美国 San Jose
e14 <- chart4 |>
  e_charts(hour, height = 60) |>
  e_line(`San Jose`, color = "#fac858", smooth = TRUE) |>
  e_y_axis(show = FALSE) |>
  e_legend(show = FALSE) |>
  e_title(
    text = "San Jose",
    textStyle = list(
      fontWeight = "bold",
      fontSize = 20
    ),
    subtext = "美国",
    subtextStyle = list(
      fontWeight = "normal",
      fontSize = 20
    ),
    left = "left",
    top = 5,
    itemGap = 5
  ) |>
  e_tooltip(
    trigger = "axis"
  ) |>
  e_grid(bottom = 20, top = 0)

# 美国 Buffalo
e15 <- chart4 |>
  e_charts(hour, height = 60) |>
  e_line(Buffalo, color = "#fac858", smooth = TRUE) |>
  e_y_axis(show = FALSE) |>
  e_legend(show = FALSE) |>
  e_title(
    text = "Buffalo",
    textStyle = list(
      fontWeight = "bold",
      fontSize = 20
    ),
    subtext = "美国",
    subtextStyle = list(
      fontWeight = "normal",
      fontSize = 20
    ),
    left = "left",
    top = 5,
    itemGap = 5
  ) |>
  e_tooltip(
    trigger = "axis"
  ) |>
  e_grid(bottom = 20, top = 0)

# 巴西 São Paulo
colnames(chart4)[21] <- "Sao Paulo"
e16 <- chart4 |>
  e_charts(hour, height = 60) |>
  e_line(`Sao Paulo`, color = "#ee6666", smooth = TRUE) |>
  e_y_axis(show = FALSE) |>
  e_legend(show = FALSE) |>
  e_title(
    text = "Sao Paulo",
    textStyle = list(
      fontWeight = "bold",
      fontSize = 20
    ),
    subtext = "巴西",
    subtextStyle = list(
      fontWeight = "normal",
      fontSize = 20
    ),
    left = "left",
    top = 5,
    itemGap = 5
  ) |>
  e_tooltip(
    trigger = "axis"
  ) |>
  e_grid(bottom = 20, top = 0)

# 巴西 Rio de Janeiro
e17 <- chart4 |>
  e_charts(hour, height = 60) |>
  e_line(`Rio de Janeiro`, color = "#ee6666", smooth = TRUE) |>
  e_y_axis(show = FALSE) |>
  e_legend(show = FALSE) |>
  e_title(
    text = "Rio de Janeiro",
    textStyle = list(
      fontWeight = "bold",
      fontSize = 20
    ),
    subtext = "巴西",
    subtextStyle = list(
      fontWeight = "normal",
      fontSize = 20
    ),
    left = "left",
    top = 5,
    itemGap = 5
  ) |>
  e_tooltip(
    trigger = "axis"
  ) |>
  e_grid(bottom = 20, top = 0)

# 巴西 Porto Alegre
e18 <- chart4 |>
  e_charts(hour, height = 60) |>
  e_line(`Porto Alegre`, color = "#ee6666", smooth = TRUE) |>
  e_y_axis(show = FALSE) |>
  e_legend(show = FALSE) |>
  e_title(
    text = "Porto Alegre",
    textStyle = list(
      fontWeight = "bold",
      fontSize = 20
    ),
    subtext = "巴西",
    subtextStyle = list(
      fontWeight = "normal",
      fontSize = 20
    ),
    left = "left",
    top = 5,
    itemGap = 5
  ) |>
  e_tooltip(
    trigger = "axis"
  ) |>
  e_grid(bottom = 20, top = 0)

# 巴西 Belo Horizonte
e19 <- chart4 |>
  e_charts(hour, height = 60) |>
  e_line(`Belo Horizonte`, color = "#ee6666", smooth = TRUE) |>
  e_y_axis(show = FALSE) |>
  e_legend(show = FALSE) |>
  e_title(
    text = "Belo Horizonte",
    textStyle = list(
      fontWeight = "bold",
      fontSize = 20
    ),
    subtext = "巴西",
    subtextStyle = list(
      fontWeight = "normal",
      fontSize = 20
    ),
    left = "left",
    top = 5,
    itemGap = 5
  ) |>
  e_tooltip(
    trigger = "axis"
  ) |>
  e_grid(bottom = 20, top = 0)

# 巴西 Brasília
e20 <- chart4 |>
  e_charts(hour, height = 60) |>
  e_line(`Brasília`, color = "#ee6666", smooth = TRUE) |>
  e_y_axis(show = FALSE) |>
  e_legend(show = FALSE) |>
  e_title(
    text = "Brasília",
    textStyle = list(
      fontWeight = "bold",
      fontSize = 20
    ),
    subtext = "巴西",
    subtextStyle = list(
      fontWeight = "normal",
      fontSize = 20
    ),
    left = "left",
    top = 5,
    itemGap = 5
  ) |>
  e_tooltip(
    trigger = "axis"
  ) |>
  e_grid(bottom = 20, top = 0)


# 伊朗 Tehran
e21 <- chart4 |>
  e_charts(hour, height = 60) |>
  e_line(Tehran, color = "#73c0de", smooth = TRUE) |>
  e_y_axis(show = FALSE) |>
  e_legend(show = FALSE) |>
  e_title(
    text = "Tehran",
    textStyle = list(
      fontWeight = "bold",
      fontSize = 20
    ),
    subtext = "伊朗",
    subtextStyle = list(
      fontWeight = "normal",
      fontSize = 20
    ),
    left = "left",
    top = 5,
    itemGap = 5
  ) |>
  e_tooltip(
    trigger = "axis"
  ) |>
  e_grid(bottom = 20, top = 0)

# 伊朗 Tiran
e22 <- chart4 |>
  e_charts(hour, height = 60) |>
  e_line(Tiran, color = "#73c0de", smooth = TRUE) |>
  e_y_axis(show = FALSE) |>
  e_legend(show = FALSE) |>
  e_title(
    text = "Tiran",
    textStyle = list(
      fontWeight = "bold",
      fontSize = 20
    ),
    subtext = "伊朗",
    subtextStyle = list(
      fontWeight = "normal",
      fontSize = 20
    ),
    left = "left",
    top = 5,
    itemGap = 5
  ) |>
  e_tooltip(
    trigger = "axis"
  ) |>
  e_grid(bottom = 20, top = 0)

# 伊朗 Isfahan
e23 <- chart4 |>
  e_charts(hour, height = 60) |>
  e_line(Isfahan, color = "#73c0de", smooth = TRUE) |>
  e_y_axis(show = FALSE) |>
  e_legend(show = FALSE) |>
  e_title(
    text = "Isfahan",
    textStyle = list(
      fontWeight = "bold",
      fontSize = 20
    ),
    subtext = "伊朗",
    subtextStyle = list(
      fontWeight = "normal",
      fontSize = 20
    ),
    left = "left",
    top = 5,
    itemGap = 5
  ) |>
  e_tooltip(
    trigger = "axis"
  ) |>
  e_grid(bottom = 20, top = 0)

# 伊朗 Tabriz
e24 <- chart4 |>
  e_charts(hour, height = 60) |>
  e_line(Tabriz, color = "#73c0de", smooth = TRUE) |>
  e_y_axis(show = FALSE) |>
  e_legend(show = FALSE) |>
  e_title(
    text = "Tabriz",
    textStyle = list(
      fontWeight = "bold",
      fontSize = 20
    ),
    subtext = "伊朗",
    subtextStyle = list(
      fontWeight = "normal",
      fontSize = 20
    ),
    left = "left",
    top = 5,
    itemGap = 5
  ) |>
  e_tooltip(
    trigger = "axis"
  ) |>
  e_grid(bottom = 20, top = 0)

# 伊朗 Mashhad
e25 <- chart4 |>
  e_charts(hour, height = 60) |>
  e_line(Mashhad, color = "#73c0de", smooth = TRUE) |>
  e_y_axis(show = FALSE) |>
  e_legend(show = FALSE) |>
  e_title(
    text = "Mashhad",
    textStyle = list(
      fontWeight = "bold",
      fontSize = 20
    ),
    subtext = "伊朗",
    subtextStyle = list(
      fontWeight = "normal",
      fontSize = 20
    ),
    left = "left",
    top = 5,
    itemGap = 5
  ) |>
  e_tooltip(
    trigger = "axis"
  ) |>
  e_grid(bottom = 20, top = 0)

e_arrange(
  e1, e2, e3, e4, e5, e6, e7,
  e8, e9, e10, e11,
  e12, e13, e14, e15, e16, e17,
  e18, e19, e20, e21,
  e22, e23, e24, e25,
  cols = 1
)
```

为了方便比对不同国家的城市或地区的情况，我们将25个折线图拼贴到一起。除了美国以外，其余四个国家的流量都有明显的波峰和波谷，并且各国出现波峰波谷的时间点是错位的，这说明"时间"这个字段并不是记录的属于各个国家时区的时间，而是一个统一的时区的时间。在 Sci-Hub 提供的下载日志数据中，时间字段也确实是没有标示出时区的。由此可知，数据中的时间字段应该是"下载论文"这一行为在该网站的后台服务器留下记录的时间，即数据中所有时间的时区统一都是服务器所在地区的时区。既然如此，我们不妨根据现实中这几个国家的时区差异，尝试将时间轴进行推移，看看是不是能得到更加符合常识的现象。

-   中国几个城市或地区的三个峰值时间点是"05:00 - 11:00 - 16:00"，若将时间轴推后5个小时变成"10:00 - 16:00 - 21:00"，正好符合国人"上午 - 下午 - 晚上"读书或工作的习惯。

-   印度几个城市或地区的两个峰值时间点是"09:00 - 12:00" ，考虑到印度比中国慢2.5小时，将时间轴推后2.5小时变成"11:30 - 14:30"，也算是在白天读书或工作。

-   巴西几个城市或地区的两个峰值时间点是"16:00 - 21:00 - 03:00"，考虑到巴西比中国慢11个小时，将时间轴提前6小时变成"10:00 - 15:00 - 21:00"，也正好符合"上午 - 下午 - 晚上"读书或工作的一般规律。

-   伊朗只有一个较为明显的高峰时间点"10:00"，考虑到伊朗比中国慢3.5小时，将时间轴推后1.5小时变成"11:30"，也算是有白天读书或工作的高峰，之后的时间不再出现波峰，也许伊朗人贡献了"11:30"的高峰流量后也继续发愤图强努力读论文。

综合看起来，除了美国几个城市的流量平平无奇中透露着诡异以外，其他几个国家的城市或地区的流量都算是较为自然的。可是，定西市的流量究竟来源何处呢？

# 3. 定西流量来源

细心的坛友[张桐川](https://github.com/tcgriffith)发现，全中国总下载量排名靠前的城市或地区里面没有深圳，因而怀疑深圳的流量被误认为"定西"。这项怀疑是合理的，只是难以通过现有数据来验证。

| 排名 | 城市或地区 | 2017年 Sci-Hub 总下载量 | 排名 | 城市或地区 | 2017年 Sci-Hub 总下载量 | 排名 | 城市或地区 | 2017年 Sci-Hub 总下载量 |
|:------:|:------:|:------:|:------:|:------:|:------:|:------:|:------:|:------:|
|  1   |    北京    |         3807399         |  11  |    长沙    |         757669          |  21  |   哈尔滨   |         373054          |
|  2   |    上海    |         2915865         |  12  |    天津    |         670712          |  22  |    南昌    |         351555          |
|  3   |  **定西**  |         2147668         |  13  |    西安    |         669085          |  23  |    青岛    |         334013          |
|  4   |    台湾    |         1999026         |  14  |    郑州    |         538862          |  24  |    福州    |         299192          |
|  5   |    香港    |         1617000         |  15  |    合肥    |         512964          |  25  |    苏州    |         271165          |
|  6   |    广州    |         1365568         |  16  |    济南    |         502566          |  26  |    大连    |         247273          |
|  7   |    南京    |         1305058         |  17  |    重庆    |         490612          |  27  |   张家口   |         228046          |
|  8   |    武汉    |         1050703         |  18  |    长春    |         457532          |  28  |    昆明    |         217803          |
|  9   |    杭州    |         962155          |  19  |    兰州    |         408890          |  29  |    南宁    |         175768          |
|  10  |    成都    |         904620          |  20  |    沈阳    |         376430          |  30  |    温州    |         144396          |

耐心的编辑[湘云](https://xiangyun.rbind.io/)建议对数据质量做更细致的校验，或可根据经纬度来反推流量所属城市，遗憾的是 Sci-Hub 的经度、纬度数据很粗犷，整个定西市的经纬度才两个。一般来说，一个用户使用一台设备登录 Sci-Hub 网站下载文献后会留下一个与设备绑定的 IP 地址。将全量数据按照 ip（IP 地址）进行分组，分别计算每个 IP 地址的 freq（总下载量），以及去重后的 user（用户 id）、city（城市）、country（国家或地区）、date（日期）、doi、 lat（纬度）、 lon（经度）的数量。如下可知，每一个 IP 地址只会对应一个城市、国家或地区、经纬度，这说明所有地理位置相关信息确实都是根据 IP 地址得来的。假如 IP 地址解析失误，那么地理层面的数据分析都会建立在这个“失误”上。

```
       ip                freq                user                city      country       date              doi                lat         lon  
 Min.   :       1   Min.   :     1.0   Min.   :     1.00   Min.   :1   Min.   :1   Min.   :  1.000   Min.   :     1.0   Min.   :1   Min.   :1  
 1st Qu.: 4199748   1st Qu.:     1.0   1st Qu.:     1.00   1st Qu.:1   1st Qu.:1   1st Qu.:  1.000   1st Qu.:     1.0   1st Qu.:1   1st Qu.:1  
 Median : 8462909   Median :     2.0   Median :     1.00   Median :1   Median :1   Median :  1.000   Median :     2.0   Median :1   Median :1  
 Mean   : 8503853   Mean   :    10.1   Mean   :     3.73   Mean   :1   Mean   :1   Mean   :  2.605   Mean   :     9.1   Mean   :1   Mean   :1  
 3rd Qu.:12786557   3rd Qu.:     5.0   3rd Qu.:     2.00   3rd Qu.:1   3rd Qu.:1   3rd Qu.:  2.000   3rd Qu.:     4.0   3rd Qu.:1   3rd Qu.:1  
 Max.   :17208864   Max.   :404071.0   Max.   :170723.00   Max.   :1   Max.   :1   Max.   :329.000   Max.   :390389.0   Max.   :1   Max.   :1  

```

如上，2017年整年的 Sci-Hub 下载日志只记录了329天的数据，去重 doi 数量最大值是390389，除以329天，四舍五入得到1187，这说明其中一个 IP 地址代表的设备平均每天会下载1187篇文献来看，显然不像是正常人类会做的事。到这里，虽然并未解开定西流量来源之谜，但有一点是肯定的，只看2017年的数据是无法得到一个确切答案的。此时不妨看看 Sci-Hub 网站提供的更早时间段的数据。

| 排名 | 城市或地区 | 2015.9-2016.2 Sci-Hub 总下载量 | 排名 | 城市或地区 | 2015.9-2016.2 Sci-Hub 总下载量 | 排名 | 城市或地区 | 2015.9-2016.2 Sci-Hub 总下载量 |
|:------:|:------:|:------:|:------:|:------:|:------:|:------:|:------:|:------:|
|  1   |  上海  |  764397  |  11  |  西安   |  109692 |  21  |  青岛  |  57635  |
|  2   |  北京  |  746797  |  12  |  济南   |  98137  |  22  |  南昌  |  55132  |
|  3   |  广州  |  230957  |  13  |  郑州   |  95114  |  23  |  福州  |  52028  |
|  4   |  武汉  |  227827  |  14  |**定西** |  91478  |  24  |  昆明  |  49704  |
|  5   |  南京  |  192035  |  15  |  沈阳   |  73951  |  25  |  合肥  |  47627  |
|  6   |  成都  |  162158  |  16  |  哈尔滨 |  72754  |  26  |  温州  |  45232  |
|  7   |**兰州**|  134120  |  17  |  长春   |  72306  |  27  |  苏州  |  43259  |
|  8   |  长沙  |  131589  |  18  |  重庆   |  66651  |  28  |  无锡  |  37389  |
|  9   |  天津  |  123871  |  19  |  杭州   |  65674  |  29  |  大连  |  37050  |
|  10  |  香港  |  111129  |  20  |  台湾   |  59091  |  30  |  东莞  |  35297  |

将2017年的数据与2015年9月至2016年2月的数据进行对比，用各城市或地区的“2017年 Sci-Hub 总下载量”除以12得到该时间段内的月均下载量，再用各城市或地区的“2015.9-2016.2 Sci-Hub 总下载量”除以6也得到该时间段内的月均下载量，两者相除得到每个城市的月均下载量上涨的倍数。整个 Sci-Hub 网站在2015年9月至2016年2月的月均下载量是463661，在2017年的月均下载量是12572989，后者除以前者得到两个时间段内月均下载量的上涨倍数是2.712。月均下载量的上涨，极有可能是因为Sci-Hub 越来越为人所知。

| 2017年排名 | 城市或地区 | 上涨倍数 | 2017年排名 | 城市或地区 | 上涨倍数 | 2017年排名 | 城市或地区 | 上涨倍数 |
|:------:|:------:|:------:|:------:|:------:|:------:|:------:|:------:|:------:|
|  1  |   北京   |   2.55    |  11  |   长沙   |  2.88  |  21  |  哈尔滨  |  2.56  |
|  2  |   上海   |   1.91    |  12  |   天津   |  2.71  |  22  |  南昌    |  3.19  |
|  3  | **定西** | **11.74** |  13  |   西安   |  3.05  |  23  |  青岛    |  2.9   |
|  4  | **台湾** | **16.91** |  14  |   郑州   |  2.83  |  24  |  福州    |  2.88  |
|  5  | **香港** | **7.28**  |  15  |   合肥   |  5.39  |  25  |  苏州    |  3.13  |
|  6  |   广州   |   2.96    |  16  |   济南   |  2.56  |  26  |  大连    |  3.34  |
|  7  |   南京   |   3.4     |  17  |   重庆   |  3.68  |  27  |  张家口  |  4.04  |
|  8  |   武汉   |   2.31    |  18  |   长春   |  3.16  |  28  |  昆明    |  2.19  |
|  9  | **杭州** | **7.33**  |  19  | **兰州** |  1.52  |  29  |  南宁    |  3.06  |
|  10 |   成都   |   2.79    |  20  |   沈阳   |  2.55  |  30  |  温州    |  1.6   |

如上表所示，定西、台湾、香港、杭州的上涨倍数远高于其他城市或地区，而兰州的上涨倍数最低。台湾和杭州的上涨倍数高，有可能是因为原来基数低。兰州的上涨倍数低，有可能是因为兰州的部分流量被误认为定西。在2015年9月至2016年2月，定西的月均下载量是15246，兰州的月均下载量是22353，即使在15246的基础上上涨3倍，再加上22353的1倍，也只有68091，仍然远不及2017年定西的月均下载量178972。在2015年9月至2016年2月，除开下载量排名前30的城市或地区，剩下的下载量一共是535288，除以6得到月均下载量为89215。假如这些原来排名前30之外的城市或地区，全都有1倍的流量在2017年被误认为定西流量，即在68091的基础上加上89215，也仍然低于2017年定西的月均下载量178972。这说明，定西的流量中一定混入了大量除中国以外的流量。

仍然以 IP 为主体，分别计算在两个时间段内每个 IP 登录网站的天数，将天数分成“1天”、“2天”、“3天”、“4-7天”、“8-15天”、“16-30天”、“30天以上”等7个层次，接着汇总各个层次的IP总数以及贡献的流量。可以看到，从“2015.9-2016.2”到“2017”，仅登录过1天的 IP 占比从76.36%降到了65.67%，其贡献的流量占比也从33.34%降到了14.84%，而登录过30天以上的 IP 占比从0.9%变到0.99%，其贡献的流量从18.39%升至46.78%。如果说仅登录过1天的流量代表的是只对 Sci-Hub 这个网站感到好奇的用户，那么登录超过30天的流量代表的就是真正长期使用 Sci-Hub 网站的用户。一般来说一个城市里长期进行科研工作需要查阅文献的人的数量不会大变，他们每年阅读文献的量应该也不会大变，而定西流量中感到好奇的用户贡献的流量大降，长期使用的用户贡献的流量大增，恰恰说明定西流量上涨的根源是更多来自其他国家的人使用了定西 IP 来登录 Sci-Hub 网站。

```{r}
data9 <- fread("data/city_divide_ip_freq.csv", encoding = "UTF-8")

colnames(data9)[c(4, 6)] <- c("IP数", "流量")

e1 <- data9[city == "Dingxi Shi" & type == "2017", ] |>
  e_charts(divide, height = 300) |>
  e_pie(`IP数`,
    radius = c("30%", "50%"),
    itemStyle = list(
      borderRadius = 20,
      borderColor = "#fff",
      borderWidth = 2
    ), center = c("30%", "50%")
  ) |>
  e_pie(`流量`,
    radius = c("30%", "50%"),
    itemStyle = list(
      borderRadius = 20,
      borderColor = "#fff",
      borderWidth = 2
    ), center = c("70%", "50%")
  ) |>
  e_labels(
    position = "outside",
    distanceToLabelLine = 1,
    formatter = "{a}：{c} \n 占比：{d}%"
  ) |>
  e_title(text = " 定西 \n\n 2017", left = 20)

e2 <- data9[city == "Dingxi Shi" & type == "2015.9-2016.2", ] |>
  e_charts(divide, height = 300) |>
  e_pie(`IP数`,
    radius = c("30%", "50%"),
    itemStyle = list(
      borderRadius = 20,
      borderColor = "#fff",
      borderWidth = 2 # 新版本该设置下画出来的是圆角环形图
    ), center = c("30%", "50%")
  ) |>
  e_pie(`流量`,
    radius = c("30%", "50%"),
    itemStyle = list(
      borderRadius = 20,
      borderColor = "#fff",
      borderWidth = 2 # 新版本该设置下画出来的是圆角环形图
    ), center = c("70%", "50%")
  ) |>
  e_labels(
    position = "outside",
    distanceToLabelLine = 1,
    formatter = "{a}：{c} \n 占比：{d}%"
  ) |>
  e_title(text = " 定西 \n\n 2015.9-2016.2", left = 20)

e_arrange(e2, e1, cols = 1)
```

# 4. 第三个波峰有多特别

Sci-Hub 2017年下载日志数据中提供的 user（用户）和 ip（IP地址）是脱敏后用数字表示的，两者之间现存在一对一、一对多、多对一、多对多这四种关系。

<!-- 假如在一台设备上登录 Sci-Hub 网站 后同一天连续下载文献会被记录为同一个用户，而在同一台设备上登录 Sci-Hub 网站后跳出再登录会被记录为另一个用户。 -->

先以 IP 为主体，拆分出“仅对应一个用户的 IP”和“对应多个用户的 IP”这两种情况，观察北京、上海、定西、台湾、香港等五个城市的每日下载量随时间推移而变化的趋势，以及定西每个月的24小时流量变化趋势。显然，“对应多个用户的 IP”所贡献的流量远多于“仅对应一个用户的 IP”所贡献的流量。在“对应多个用户的 IP”条件下，各个城市或地区的24小时流量变化趋势与没拆分前差异不大。奇怪的是在“仅对应一个用户的 IP”条件下，不止定西，连北京、上海的24小时流量变化趋势也会出现第三个波峰最高的情况。

```{r}
# freq1:user_label==1，仅对应一个IP的user
# freq2:user_label==2，对应多个IP的user
# freq3:ip_label==1，仅对应一个user的IP
# freq4:ip_label==2，对应多个user的IP

# 按天
data5 <- fread("data/cn_by_day_ip_user.csv", encoding = "UTF-8")

data5$date <- as.Date(data5$date)

e1 <- data5 |>
  group_by(type) |>
  e_charts(date, height = 300, timeline = TRUE) |>
  e_line(freq3, name = "仅对应一个用户的IP") |>
  e_line(freq4, name = "对应多个用户的IP") |>
  e_tooltip(trigger = "axis") |>
  e_y_axis(name = "每日下载量", axisLine = list(
    show = TRUE,
    symbol = c("none", "arrow")
  )) |>
  e_legend(top = 40) |>
  e_title(left = "center", top = 5) |>
  e_timeline_serie(title = list(
    list(
      text = "北京 Sci-Hub 2017年下载量"
    ),
    list(text = "定西 Sci-Hub 2017年下载量"),
    list(text = "上海 Sci-Hub 2017年下载量"),
    list(text = "台湾 Sci-Hub 2017年下载量"),
    list(text = "香港 Sci-Hub 2017年下载量")
  ))

# 按小时
data6 <- fread("data/cn_by_month_hour_ip_user.csv", encoding = "UTF-8")

data6.freq3 <- data6[, c("month", "hour", "freq3", "type")]
data6.freq3 <- dcast(data6.freq3, month + hour ~ type, value.var = "freq3")

data6.freq3 <- data6.freq3[order(hour), ]
data6.freq3$hour <- as.character(data6.freq3$hour)

e2 <- data6.freq3 |>
  group_by(month) |>
  e_charts(hour, height = 300, timeline = TRUE) |>
  e_line(`北京`, smooth = TRUE) |>
  e_line(`上海`, smooth = TRUE) |>
  e_line(`定西`, smooth = TRUE) |>
  e_line(`台湾`, smooth = TRUE) |>
  e_line(`香港`, smooth = TRUE) |>
  e_tooltip(trigger = "axis") |>
  e_y_axis(name = "24小时下载量", axisLine = list(
    show = TRUE,
    symbol = c("none", "arrow")
  )) |>
  e_legend(
    right = 30,
    top = "middle",
    selector = c("all", "inverse"),
    selectorPosition = "start",
    orient = "vertical",
    itemGap = 15
  ) |>
  e_title(left = "center", top = 5) |>
  e_timeline_serie(title = list(
    list(
      text = "仅对应一个用户的IP",
      textStyle = list(
        fontWeight = "normal",
        fontSize = 20
      ),
      subtext = "1月",
      subtextStyle = list(
        fontWeight = "bold",
        fontSize = 30
      )
    ),
    list(text = "仅对应一个用户的IP", subtext = "2月"),
    list(text = "仅对应一个用户的IP", subtext = "3月"),
    list(text = "仅对应一个用户的IP", subtext = "4月"),
    list(text = "仅对应一个用户的IP", subtext = "5月"),
    list(text = "仅对应一个用户的IP", subtext = "6月"),
    list(text = "仅对应一个用户的IP", subtext = "7月"),
    list(text = "仅对应一个用户的IP", subtext = "8月"),
    list(text = "仅对应一个用户的IP", subtext = "9月"),
    list(text = "仅对应一个用户的IP", subtext = "10月"),
    list(text = "仅对应一个用户的IP", subtext = "11月"),
    list(text = "仅对应一个用户的IP", subtext = "12月")
  ))

data6.freq4 <- data6[, c("month", "hour", "freq4", "type")]
data6.freq4 <- dcast(data6.freq4, month + hour ~ type, value.var = "freq4")

data6.freq4 <- data6.freq4[order(hour), ]
data6.freq4$hour <- as.character(data6.freq4$hour)

e3 <- data6.freq4 |>
  group_by(month) |>
  e_charts(hour, height = 300, timeline = TRUE) |>
  e_line(`北京`, smooth = TRUE) |>
  e_line(`上海`, smooth = TRUE) |>
  e_line(`定西`, smooth = TRUE) |>
  e_line(`台湾`, smooth = TRUE) |>
  e_line(`香港`, smooth = TRUE) |>
  e_tooltip(trigger = "axis") |>
  e_y_axis(name = "24小时下载量", axisLine = list(
    show = TRUE,
    symbol = c("none", "arrow")
  )) |>
  e_legend(
    right = 30,
    top = "middle",
    selector = c("all", "inverse"),
    selectorPosition = "start",
    orient = "vertical",
    itemGap = 15
  ) |>
  e_title(left = "center", top = 5) |>
  e_timeline_serie(title = list(
    list(
      text = "对应多个用户的IP",
      textStyle = list(
        fontWeight = "normal",
        fontSize = 20
      ),
      subtext = "1月",
      subtextStyle = list(
        fontWeight = "bold",
        fontSize = 30
      )
    ),
    list(text = "对应多个用户的IP", subtext = "2月"),
    list(text = "对应多个用户的IPP", subtext = "3月"),
    list(text = "对应多个用户的IPP", subtext = "4月"),
    list(text = "对应多个用户的IP", subtext = "5月"),
    list(text = "对应多个用户的IPP", subtext = "6月"),
    list(text = "对应多个用户的IP", subtext = "7月"),
    list(text = "对应多个用户的IP", subtext = "8月"),
    list(text = "对应多个用户的IP", subtext = "9月"),
    list(text = "对应多个用户的IP", subtext = "10月"),
    list(text = "对应多个用户的IP", subtext = "11月"),
    list(text = "对应多个用户的IP", subtext = "12月")
  ))

e_arrange(e1, e2, e3, cols = 1)
```

再以用户为主体，拆分出"仅对应一个 IP 的用户"和"对应多个 IP 的用户"这两种情况，观察北京、上海、定西、台湾、香港等五个城市的每日下载量随时间推移而变化的趋势。

-   北京、台湾：在2017年1-10月"对应多个 IP 的用户"所贡献的流量比"仅对应一个 IP 的用户"平均多0.5倍，在2017年11-12月两种情况贡献的流量几乎持平。
-   上海：两种情况的数据表现多有重叠、所贡献的流量差别不大。
-   定西：与北京、台湾相似，在2017年1-7月"对应多个 IP 的用户"所贡献的流量比"仅对应一个 IP 的用户"多1-2倍，随后两种情况之间的差异逐渐缩小，截止12月两种情况贡献的流量几乎持平。
-   香港：两种情况走的是"三十年河东、三十年河西"路线。

接着观察各城市或地区每个月的24小时流量变化趋势，北京、上海、台湾、香港等4个城市或地区的两种情况表现一致，都是每个月的第三个波峰最低。而定西市的情况是："对应多个 IP 的用户"的流量一直都是第三个波峰最高；"仅对应一个 IP 的用户"的流量在1-10月是第三个波峰最低，在11-12月变成5点-10点-16点三个波峰的高度大致持平。

```{r}
e1 <- data5 |>
  group_by(type) |>
  e_charts(date, height = 300, timeline = TRUE) |>
  e_line(freq1, name = "仅对应一个IP的用户") |>
  e_line(freq2, name = "对应多个IP的用户") |>
  e_tooltip(trigger = "axis") |>
  e_y_axis(name = "每日下载量", axisLine = list(
    show = TRUE,
    symbol = c("none", "arrow")
  )) |>
  e_legend(top = 40) |>
  e_title(left = "center", top = 5) |>
  e_timeline_serie(title = list(
    list(
      text = "北京 Sci-Hub 2017年下载量"
    ),
    list(text = "定西 Sci-Hub 2017年下载量"),
    list(text = "上海 Sci-Hub 2017年下载量"),
    list(text = "台湾 Sci-Hub 2017年下载量"),
    list(text = "香港 Sci-Hub 2017年下载量")
  ))

data6 <- data6[order(hour), ]
data6$hour <- as.character(data6$hour)

e2 <- data6[type == "定西", ] |>
  group_by(month) |>
  e_charts(hour, height = 300, timeline = TRUE) |>
  e_line(freq1, smooth = TRUE, name = "仅对应一个IP的用户") |>
  e_line(freq2, smooth = TRUE, name = "对应多个IP的用户") |>
  e_tooltip(trigger = "axis") |>
  e_y_axis(name = "24小时下载量", axisLine = list(
    show = TRUE,
    symbol = c("none", "arrow")
  )) |>
  e_legend(
    right = 30,
    orient = "vertical",
    itemGap = 15
  ) |>
  e_title(left = "center", top = 5) |>
  e_timeline_serie(title = list(
    list(
      text = "定西",
      textStyle = list(
        fontWeight = "normal",
        fontSize = 20
      ),
      subtext = "1月",
      subtextStyle = list(
        fontWeight = "bold",
        fontSize = 30
      )
    ),
    list(text = "定西", subtext = "2月"),
    list(text = "定西", subtext = "3月"),
    list(text = "定西", subtext = "4月"),
    list(text = "定西", subtext = "5月"),
    list(text = "定西", subtext = "6月"),
    list(text = "定西", subtext = "7月"),
    list(text = "定西", subtext = "8月"),
    list(text = "定西", subtext = "9月"),
    list(text = "定西", subtext = "10月"),
    list(text = "定西", subtext = "11月"),
    list(text = "定西", subtext = "12月")
  ))

e3 <- data6[type == "北京", ] |>
  group_by(month) |>
  e_charts(hour, height = 300, timeline = TRUE) |>
  e_line(freq1, smooth = TRUE, name = "仅对应一个IP的用户") |>
  e_line(freq2, smooth = TRUE, name = "对应多个IP的用户") |>
  e_tooltip(trigger = "axis") |>
  e_y_axis(name = "24小时下载量", axisLine = list(
    show = TRUE,
    symbol = c("none", "arrow")
  )) |>
  e_legend(
    right = 30,
    orient = "vertical",
    itemGap = 15
  ) |>
  e_title(left = "center", top = 5) |>
  e_timeline_serie(title = list(
    list(
      text = "北京",
      textStyle = list(
        fontWeight = "normal",
        fontSize = 20
      ),
      subtext = "1月",
      subtextStyle = list(
        fontWeight = "bold",
        fontSize = 30
      )
    ),
    list(text = "北京", subtext = "2月"),
    list(text = "北京", subtext = "3月"),
    list(text = "北京", subtext = "4月"),
    list(text = "北京", subtext = "5月"),
    list(text = "北京", subtext = "6月"),
    list(text = "北京", subtext = "7月"),
    list(text = "北京", subtext = "8月"),
    list(text = "北京", subtext = "9月"),
    list(text = "北京", subtext = "10月"),
    list(text = "北京", subtext = "11月"),
    list(text = "北京", subtext = "12月")
  ))

e4 <- data6[type == "上海", ] |>
  group_by(month) |>
  e_charts(hour, height = 300, timeline = TRUE) |>
  e_line(freq1, smooth = TRUE, name = "仅对应一个IP的用户") |>
  e_line(freq2, smooth = TRUE, name = "对应多个IP的用户") |>
  e_tooltip(trigger = "axis") |>
  e_y_axis(name = "24小时下载量", axisLine = list(
    show = TRUE,
    symbol = c("none", "arrow")
  )) |>
  e_legend(
    right = 30,
    orient = "vertical",
    itemGap = 15
  ) |>
  e_title(left = "center", top = 5) |>
  e_timeline_serie(title = list(
    list(
      text = "上海",
      textStyle = list(
        fontWeight = "normal",
        fontSize = 20
      ),
      subtext = "1月",
      subtextStyle = list(
        fontWeight = "bold",
        fontSize = 30
      )
    ),
    list(text = "上海", subtext = "2月"),
    list(text = "上海", subtext = "3月"),
    list(text = "上海", subtext = "4月"),
    list(text = "上海", subtext = "5月"),
    list(text = "上海", subtext = "6月"),
    list(text = "上海", subtext = "7月"),
    list(text = "上海", subtext = "8月"),
    list(text = "上海", subtext = "9月"),
    list(text = "上海", subtext = "10月"),
    list(text = "上海", subtext = "11月"),
    list(text = "上海", subtext = "12月")
  ))

e5 <- data6[type == "台湾", ] |>
  group_by(month) |>
  e_charts(hour, height = 300, timeline = TRUE) |>
  e_line(freq1, smooth = TRUE, name = "仅对应一个IP的用户") |>
  e_line(freq2, smooth = TRUE, name = "对应多个IP的用户") |>
  e_tooltip(trigger = "axis") |>
  e_y_axis(name = "24小时下载量", axisLine = list(
    show = TRUE,
    symbol = c("none", "arrow")
  )) |>
  e_legend(
    right = 30,
    orient = "vertical",
    itemGap = 15
  ) |>
  e_title(left = "center", top = 5) |>
  e_timeline_serie(title = list(
    list(
      text = "台湾",
      textStyle = list(
        fontWeight = "normal",
        fontSize = 20
      ),
      subtext = "1月",
      subtextStyle = list(
        fontWeight = "bold",
        fontSize = 30
      )
    ),
    list(text = "台湾", subtext = "2月"),
    list(text = "台湾", subtext = "3月"),
    list(text = "台湾", subtext = "4月"),
    list(text = "台湾", subtext = "5月"),
    list(text = "台湾", subtext = "6月"),
    list(text = "台湾", subtext = "7月"),
    list(text = "台湾", subtext = "8月"),
    list(text = "台湾", subtext = "9月"),
    list(text = "台湾", subtext = "10月"),
    list(text = "台湾", subtext = "11月"),
    list(text = "台湾", subtext = "12月")
  ))

e6 <- data6[type == "香港", ] |>
  group_by(month) |>
  e_charts(hour, height = 300, timeline = TRUE) |>
  e_line(freq1, smooth = TRUE, name = "仅对应一个IP的用户") |>
  e_line(freq2, smooth = TRUE, name = "对应多个IP的用户") |>
  e_tooltip(trigger = "axis") |>
  e_y_axis(name = "24小时下载量", axisLine = list(
    show = TRUE,
    symbol = c("none", "arrow")
  )) |>
  e_legend(
    right = 30,
    orient = "vertical",
    itemGap = 15
  ) |>
  e_title(left = "center", top = 5) |>
  e_timeline_serie(title = list(
    list(
      text = "香港",
      textStyle = list(
        fontWeight = "normal",
        fontSize = 20
      ),
      subtext = "1月",
      subtextStyle = list(
        fontWeight = "bold",
        fontSize = 30
      )
    ),
    list(text = "香港", subtext = "2月"),
    list(text = "香港", subtext = "3月"),
    list(text = "香港", subtext = "4月"),
    list(text = "香港", subtext = "5月"),
    list(text = "香港", subtext = "6月"),
    list(text = "香港", subtext = "7月"),
    list(text = "香港", subtext = "8月"),
    list(text = "香港", subtext = "9月"),
    list(text = "香港", subtext = "10月"),
    list(text = "香港", subtext = "11月"),
    list(text = "香港", subtext = "12月")
  ))

e_arrange(e1, e2, e3, e4, e5, e6, cols = 1)
```

从表面上来看，定西市的"仅对应一个 IP 的用户"，在2017年前几个月份和其他城市或地区的人是一样的作息规律，到了11月、12月就变成晚上（第三个波峰）也要努力读论文；而"对应多个IP的用户"就真得一直都是晚上（第三个波峰）比上午（第一个波峰）和下午（第二个波峰）都更加努力读论文。如果这两种用户都来自同一个城市，那两种情况下贡献的流量变化趋势应该跟北京、上海等城市或地区一样表现出相似的作息规律。现在这两种情况之间的差异表明，定西流量中确实混入了大量其他时区的流量。

# 5. 流量组成

现在既不以 IP 为主体，也不以用户为主体，而是将流量拆分成四种独立情况：IP与用户一对一、IP与用户一对多、IP与用户多对一、IP与用户多对多。观察 Sci-Hub 全站流量以及北京、上海、定西、台湾、香港等5个城市或地区在这四种情况下的流量分布，下面6个饼图中比例最大的两部分都是“IP与用户一对多”和“IP与用户多对多”。

```{r}
data7 <- fread("data/hour_abcd.csv", encoding = "UTF-8")

data8 <- data7[, by = .(type), .(
  A = sum(A),
  B = sum(B),
  C = sum(C),
  D = sum(D)
)]

colnames(data8)[2:5] <- c("IP与用户一对一", "IP与用户一对多", "IP与用户多对一", "IP与用户多对多")

data8 <- melt(data8, id = 1)
data8 <- dcast(data8, variable ~ type, value.var = "value")

# radius：调整半径大小
# center：第一个值调左右，第二个值调上下
data8 |>
  e_charts(variable) |>
  e_pie(scihub, radius = "20%", center = c("30%", "20%")) |>
  e_pie(`定西`, radius = "20%", center = c("70%", "20%")) |>
  e_pie(`北京`, radius = "20%", center = c("30%", "50%")) |>
  e_pie(`上海`, radius = "20%", center = c("70%", "50%")) |>
  e_pie(`台湾`, radius = "20%", center = c("30%", "80%")) |>
  e_pie(`香港`, radius = "20%", center = c("70%", "80%")) |>
  e_labels(
    show = TRUE,
    alignTo = "labelLine",
    position = "outside",
    distanceToLabelLine = 1,
    formatter = "{a}：{c} \n 占比：{d}%"
  )
```

可还记得第3节中的68091么？就是2015年9月至2016年2月之间的定西月均下载量上涨3倍，再加上此时间段内1倍的兰州月均下载量。若是乘以12，得到817092，与“IP与用户一对多”条件下的2017年定西下载量752466很接近。有没有一种可能，在2017年的定西流量中，只有“IP与用户一对多”条件下的流量绝大部分是本来的“定西流量”和被误认为定西的兰州流量，剩下三种条件下的流量绝大部分都是其他时区的流量？观察定西在这四种条件下的24小时流量趋势，正好是只有“IP与用户一对多”条件下第三个波峰最低，其他条件下都是第三个波峰最高。

如下图，在“IP与用户一对多”条件下这5个城市或地区都是第三个波峰最低，几乎可以肯定这个条件的流量就是住在这些城市或地区的正常人类所产生的流量。在“IP与用户多对多”条件下，除了定西是第三个波峰最高，其余4个城市或地区都是第三个波峰最低。倘若北京、上海、台湾、香港在“IP与用户一对多”和“IP与用户多对多”条件下的流量绝大部分都是自然流量，那么定西在“IP与用户多对多”条件下的流量绝大部分就是来自其他时区的。

```{r}
# A:unique_ip==1&unique_user==1
# B:unique_ip==1&unique_user>1
# C:unique_ip>1&unique_user==1
# D:unique_ip>1&unique_user>1
data7 <- fread("data/hour_abcd.csv", encoding = "UTF-8")

data7 <- data7[order(hour), ]
data7$hour <- as.character(data7$hour)

data7 |>
  group_by(type) |>
  e_charts(hour, timeline = TRUE) |>
  # e_line(A, smooth = TRUE, name = "IP与用户一对一") |>
  e_line(B, smooth = TRUE, name = "IP与用户一对多") |>
  # e_line(C, smooth = TRUE, name = "IP与用户多对一") |>
  e_line(D, smooth = TRUE, name = "IP与用户多对多") |>
  e_tooltip(trigger = "axis") |>
  e_legend(
    type = "scroll",
    selector = c("all", "inverse"),
    selectorPosition = "end",
    top = 25
  ) |>
  e_y_axis(
    name = "下载量", axisLine = list(
      show = TRUE,
      symbol = c("none", "arrow")
    )
  ) |>
  e_title(text = "2017年 Sci-Hub 24小时下载量", left = "center")
```

# 6. 结尾

从数据表象来看，定西之谜的谜题有两点：

1. 定西市本身没有知名研究机构，为何在 Sci-Hub 上的下载量那么高，仅次于北京、上海？
2. 北京、上海等城市或地区的24小时流量变化趋势都是出现3个波峰，且第3个波峰最低，为何定西市三个波峰差不多高，并且第3个波峰反而是最高的？

虽然探索到最后，定西市的流量之谜并没有完全解开，但经过一番探索后得到的结论有以下几点：

1. 定西流量中约有三到四成确实来自于定西市，其中也可能包含被误认为定西的兰州流量，剩下的六到七成均来自定西之外。

2. 定西流量中占比最高的两个部分分别是“IP与用户一对多”和“IP与用户多对多”，前者贡献的是最正常的自然流量，第三个波峰最低，后者混入了大量其他时区的自然流量，第三个波峰最高，且极有可能是身在其他时区却使用定西 IP 的用户，但前者贡献的流量少于后者，于是综合起来变成了三个差不多高的波峰且第三个波峰变得最高。

若你看到这里，心中存有疑虑，或者觉得本文哪里解释不通，不妨自己动动脑、动动手，鼓捣鼓捣数据、捣鼓捣鼓工具，自行做一番探索性数据分析，或者搜集基于 Sci-Hub 网站公开数据以外的信息，也许能发现一些更有趣的东西。本（咸）文（鱼）作（人）者（类）认为，Sci-Hub 网站提供的下载日志数据虽然字段不多，但是可以拆分出极丰富的维度进行分析，对她的探索可以是无穷无尽、地老天荒、海枯石烂的。

# 7. 番外：漂亮国的无峰值数据表现

本文第2节中的24小时流量变化趋势显示，美国的 Sci-Hub 流量极有可能是由非正常手段产生的非自然流量。参照定西之谜的解题思路，也将用户分为"仅对应一个IP的用户"和"对应多个IP的用户"两种情况。观察每日下载量随时间推移的变化趋势，在4.21-4.29有数据缺失，从4.30开始不知发生了什么导致两种情况差距越来越大。

+ 对应多个 IP 的用户：数据表现十分平稳，且存在明显的周期性趋势；
+ 仅对应一个 IP 的用户：在1-4月表现平稳，此后出现极大幅度的波动。

```{r}
la.1 <- fread("data/us_1.csv") # 美国

la.1$date <- as.Date(la.1$date)
la.1 |>
  e_charts(date) |>
  e_line(freq1, name = "仅对应一个IP的用户") |>
  e_line(freq2, name = "对应多个IP的用户") |>
  e_tooltip(trigger = "axis") |>
  e_y_axis(name = "下载量", axisLine = list(
    show = TRUE,
    symbol = c("none", "arrow")
  )) |>
  e_legend(top = 40) |>
  e_title("美国 Sci-Hub 2017年每日下载量", left = "center", top = 5)
```

接着观察每个月的24小时下载量变化趋势。

-   在1-4月出现了明显的波峰波谷，三个波峰是18/19点-21/22点-5点，如果按中国时间推算要往后移5个小时才与正常作息时间相符，考虑到美国比中国慢12-13小时，那么按当前时间轴往前移8个小时，原来的三个波峰就变成10/11点-13/14点-21点，也差不多符合上午-下午-晚上读论文的作息规律。

-   在1-4月基本都是"对应多个IP的用户"所贡献的流量比"仅对应一个IP的用户"多一点，到了5月情况反转，此后"仅对应一个IP的用户"所贡献的流量一直是"对应多个IP的用户"的数倍。从5月开始"仅对应一个IP的用户"的流量变化趋势越来越平缓，明显属于非自然流量。

```{r}
la.2 <- fread("data/us_2.csv") # 美国

la.2$hour <- as.character(la.2$hour)

la.2 |>
  group_by(month) |>
  e_charts(hour, timeline = TRUE) |>
  e_line(freq1, smooth = TRUE, name = "仅对应一个IP的用户") |>
  e_line(freq2, smooth = TRUE, name = "对应多个IP的用户") |>
  e_tooltip(trigger = "axis") |>
  e_y_axis(name = "下载量", axisLine = list(
    show = TRUE,
    symbol = c("none", "arrow")
  )) |>
  e_legend(top = 40) |>
  e_title("美国 Sci-Hub 2017年24小时下载量", left = "center", top = 5)
```

总体来看，美国的流量里在1-4月仍然是自然流量为主，但是从5月开始混入大量非自然流量，最终将整体的24小时变化趋势几乎拉成了一条直线。

# 8. 运行环境

在 RStudio IDE 内编辑本文的 R Markdown 源文件，用 **blogdown** [@blogdown2017] 构建网站，[Hugo](https://github.com/gohugoio/hugo) 渲染 knitr 之后的 Markdown 文件，得益于 **blogdown** 对 R Markdown 格式的支持，图、表和参考文献的交叉引用非常方便，省了不少文字编辑功夫。文中使用了多个 R 包，**echarts4r** [@echarts4r]绘图交互图形，**data.table**[@data.table] 包处理数据，**rcrossref** [@rcrossref]获取文章元数据。

为方便复现本文内容，下面列出详细的环境信息：

```{r, message=FALSE, echo=TRUE}
xfun::session_info(packages = c(
  "knitr", "rmarkdown", "blogdown",
  "echarts4r.maps", "echarts4r", "countrycode",
  "rcrossref", "data.table"
), dependencies = FALSE)
```

```{r write-bib, include=FALSE}
pkgs <- c("echarts4r", "data.table", "echarts4r.maps", "blogdown", "rcrossref")
bib <- knitr::write_bib(
  x = pkgs, file = NULL, prefix = ""
)
bib <- unlist(bib)
bib <- gsub("(\\\n)", " ", bib)
xfun::write_utf8(bib, "packages.bib")
```

# 9. 参考文献

[^1]:Elbakyan, Alexandra. (2020). Sci-Hub download log 2011-2013 [Data set]. Zenodo. https://doi.org/10.5281/zenodo.5918542

[^2]:Elbakyan, Alexandra; Bohannon, John (2021), Data from: Who's downloading pirated papers? Everyone, Dryad, Dataset, https://doi.org/10.5061/dryad.q447c

[^3]:Alexandra Elbakyan. (2018). Sci-Hub download log of 2017 [Data set]. Zenodo. https://doi.org/10.5281/zenodo.1158301
